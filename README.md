# eecs-127-research-projects
Project descriptions for the research projects deployed in the Spring 2023 iteration of EECS 127 at UC Berkeley.

Descent Methods: This project walked students through Allen et al. [1] that illustrated how the famous Nesterov Accelerated Gradient Descent method is understood through Mirror Descent.

Adversarial Machine Learning: This project follows Wong et al. [2] to showcase how the key idea of duality from optimization could lead to provable defenses against adversarial examples in ML classification using neural networks. 

Control Theory: This project builds on Ding et al. [3] and illustrates the complexities of a simple open problem in control with multiplicative noise, while guiding students to discover new possible strategies to improve performance.

## Reference Papers
[1] Zeyuan Allen-Zhu and Lorenzo Orecchia. 2014. Linear coupling: An ultimate unification of gradient and mirror descent. arXiv preprint arXiv:1407.1537 (2014).

[2] Eric Wong and Zico Kolter. 2018. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International conference on machine learning. PMLR, 5286–5295.

[3] Jian Ding, Yuval Peres, Gireeja Ranade, and Alex Zhai. 2019. When multiplicative noise stymies control. The Annals of Applied Probability 29, 4 (2019), 1963–1992.

